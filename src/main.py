"""Main entry point for Modular RAG using LangChain and LangGraph."""
import uvicorn
import logging
import argparse
import os
from src.core.config import get_config

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def main():
    """Main entry point with LangChain/LangGraph support."""
    parser = argparse.ArgumentParser(description="Modular RAG System (LangChain + LangGraph)")
    parser.add_argument(
        "--mode",
        choices=["api", "cli", "index"],
        default="api",
        help="Run mode: api (default), cli, or index"
    )
    parser.add_argument(
        "--host",
        default="0.0.0.0",
        help="Host for API server"
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8000,
        help="Port for API server"
    )
    parser.add_argument(
        "--reload",
        action="store_true",
        help="Enable auto-reload for development"
    )
    # Index mode arguments
    parser.add_argument(
        "--path",
        help="File or directory to index (for --mode index)"
    )
    parser.add_argument(
        "--recursive",
        action="store_true",
        help="Recursively index directory"
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=1024,
        help="Chunk size for indexing"
    )
    
    args = parser.parse_args()
    
    if args.mode == "api":
        run_api(args.host, args.port, args.reload)
    elif args.mode == "cli":
        run_cli()
    elif args.mode == "index":
        if not args.path:
            parser.error("--mode index requires --path argument")
        run_index(args.path, args.recursive, args.chunk_size)


def run_api(host: str, port: int, reload: bool):
    """Run the FastAPI server with LangChain and LangGraph."""
    from src.api.main import app
    from langchain_core.callbacks import BaseCallbackHandler
    
    # Add LangChain logging callback
    class APICallbackHandler(BaseCallbackHandler):
        def on_llm_start(self, serialized, prompts, **kwargs):
            logger.info("LangChain LLM started processing")
        
        def on_llm_end(self, response, **kwargs):
            logger.info("LangChain LLM finished processing")
    
    logger.info(f"Starting LangChain/LangGraph API server on {host}:{port}")
    uvicorn.run(
        "src.api.main:app",
        host=host,
        port=port,
        reload=reload,
    )


def run_cli():
    """Run CLI mode with LangChain chains."""
    from src.components.orchestration.rag_graph import ModularRAGWorkflow
    
    workflow = ModularRAGWorkflow()
    
    print("\n=== Modular RAG CLI (LangChain + LangGraph) ===")
    print("Type 'quit' to exit\n")
    
    while True:
        try:
            query = input("You: ").strip()
            if not query:
                continue
            if query.lower() in ['quit', 'exit', 'q']:
                break
            
            # Run query through LangGraph workflow
            result = workflow.query(query)
            
            print(f"\nüìù Answer: {result['answer']}")
            print(f"üìä Confidence: {result['confidence']:.2%}")
            
            if result.get('sources'):
                print("\nüìö Sources:")
                for i, source in enumerate(result['sources'][:3], 1):
                    doc_id = source.get('document_id', 'Unknown')
                    chunk_idx = source.get('metadata', {}).get('chunk_index', '?')
                    content_preview = source.get('content', '')[:60].replace('\n', ' ')
                    print(f"  {i}. [{doc_id}] Chunk {chunk_idx}: \"{content_preview}...\"")
            
            if result.get('decomposed_queries'):
                print(f"\nüîç Sub-queries analyzed: {len(result['decomposed_queries'])}")
            
            print()
            
        except KeyboardInterrupt:
            print("\nGoodbye!")
            break
        except Exception as e:
            print(f"Error: {e}")


def run_index(path: str, recursive: bool, chunk_size: int):
    """Run indexing mode with LangChain vector store."""
    from src.components.retrieval.document_processor import DocumentProcessor, ChunkingStrategy
    from src.components.retrieval.vector_store import VectorStoreManager, LangChainChromaVectorStore
    from src.core.embedding import get_embedding
    
    embedding = get_embedding()
    vector_store = LangChainChromaVectorStore(embedding_wrapper=embedding)
    vsm = VectorStoreManager(vector_store=vector_store, embedding_wrapper=embedding)
    
    processor = DocumentProcessor(
        chunking_strategy=ChunkingStrategy.RECURSIVE,
        chunk_size=chunk_size,
        embedding_wrapper=embedding,
    )
    
    if os.path.isfile(path):
        print(f"üìÑ Processing file: {path}")
        documents = [processor.process_file(path)]
    elif os.path.isdir(path):
        print(f"üìÅ Processing directory: {path} (recursive={recursive})")
        documents = processor.process_directory(path, recursive=recursive)
    else:
        print(f"‚ùå Error: {path} is not a valid file or directory")
        return
    
    print(f"üìä Found {len(documents)} documents")
    
    total_chunks = sum(len(doc.chunks) for doc in documents if doc.chunks)
    print(f"üìë Total chunks: {total_chunks}")
    
    print("üîÑ Indexing with LangChain ChromaDB...")
    vsm.index_documents(documents)
    print(f"‚úÖ Indexed {len(documents)} documents successfully!")


if __name__ == "__main__":
    main()
