# Modular RAG Configuration
# Production-ready configuration for Modular RAG system

# Application Configuration
app:
  name: "Modular RAG"
  version: "1.0.0"
  debug: false
  log_level: "INFO"

# LLM Configuration (Ollama)
llm:
  provider: "ollama"
  model: "rnj-1:8b"
  base_url: "http://localhost:11434"
  temperature: 0.1
  max_tokens: 4096
  timeout: 120
  context_length: 8192

# Embedding Configuration (Ollama)
embedding:
  provider: "ollama"
  model: "nomic-embed-text-v2-moe"
  base_url: "http://localhost:11434"
  dimensions: 768
  batch_size: 32

# Vector Store Configuration
vector_store:
  provider: "chromadb" # Options: chromadb, faiss, qdrant
  collection_name: "modular_rag_documents"
  persist_directory: "./data/vector_store"
  distance_metric: "cosine"

# BM25 Configuration
bm25:
  enabled: true
  k1: 1.2
  b: 0.75
  top_k: 20

# Hybrid Search Configuration
hybrid_search:
  enabled: true
  fusion_method: "rrf" # Options: rrf, weighted, score_normalized
  alpha: 0.5 # Weight for dense vectors (1-alpha for sparse)
  top_k: 50

# Reranking Configuration
reranking:
  enabled: true
  method: "colbert" # Options: colbert, cross_encoder, instruction_following
  cross_encoder_model: "cross-encoder/ms-marco-MiniLM-L-12-v2"
  colbert_max_tokens: 512
  top_k: 10
  batch_size: 32
  # Temporal-aware reranking: boosts chunks with matching dates/quarters
  temporal_boost:
    enabled: true
    match_boost: 1.5      # Multiplier for exact temporal match (e.g., Q2 2025)
    mismatch_penalty: 0.7 # Multiplier for temporal mismatch (e.g., Q1 when asking for Q2)

# HyDE Configuration
hyde:
  enabled: true
  hypothesis_prompt: |
    Write a detailed hypothetical answer to the question. 
    Include specific facts, examples, and reasoning that would appear in a relevant document.
    Question: {query}
    Hypothetical Answer:
  temperature: 0.3
  max_tokens: 512

# Query Decomposition Configuration
query_decomposition:
  enabled: true
  method: "rq-rag" # Options: rq-rag, decomposition, stepback
  max_sub_questions: 5
  decomposition_prompt: |
    Decompose the following complex question into simpler sub-questions.
    Each sub-question should be answerable independently.
    Original Question: {query}
    Sub-questions:
  stepback_prompt: |
    Generate a broader, more general question that captures the context of the original query.
    This helps retrieve relevant background information.
    Original Question: {query}
    Step-back Question:

# RAPTOR Configuration
raptor:
  enabled: true
  clustering_method: "hierarchical" # Options: hierarchical, flat
  num_clusters: 5
  embedding_model: "nomic-embed-text-v2-moe"
  summarization_model: "rnj-1:8b"
  max_summary_length: 512
  threshold: 0.7

# Document Processing Configuration
document_processing:
  chunk_size: 1024
  chunk_overlap: 128
  separators: ["\n\n", "\n", ". ", "! ", "? ", "; ", " ", ""]
  length_function: "len"
  # Metadata extraction during document ingestion
  metadata_extraction:
    enabled: true
    rule_based: true          # Extract temporal, financial, entity metadata (fast)
    llm_based: false          # Extract semantic topics, summary (slower, more accurate)
    extract_temporal: true    # Quarters, years, dates
    extract_financial: true   # Currency, percentages, metrics
    extract_entities: true    # Companies, products

# Granularity-Aware Retrieval Configuration
granularity_retrieval:
  enabled: true
  long_context_compression: true
  compression_ratio: 0.5
  filter_irrelevant_spans: true
  span_filter_threshold: 0.3

# LangGraph Orchestration Configuration
langgraph:
  enabled: true
  checkpointing: true
  human_in_loop: false
  workflow_name: "modular_rag_workflow"

# API Configuration
api:
  host: "0.0.0.0"
  port: 8000
  workers: 4
  cors_origins: ["*"]
  docs_enabled: true

# Evaluation Configuration
evaluation:
  enabled: true
  metrics:
    - "retrieval_precision"
    - "retrieval_recall"
    - "answer_relevancy"
    - "faithfulness"
    - "context_precision"
  sample_size: 100

# Cache Configuration
cache:
  enabled: true
  type: "disk" # Options: disk, memory
  max_size: 1000
  ttl: 3600

# Production Settings
production:
  environment: "development" # Options: development, staging, production
  monitoring: true
  metrics_port: 9090
  health_check_interval: 30
