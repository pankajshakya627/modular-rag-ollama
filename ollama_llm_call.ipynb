{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ollama_langchain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================",
    "# Basic LangChain + Ollama Integration with rnj-1:8b model",
    "# ============================================================",
    "\n",
    "# Step 1: Import the ChatOllama class from langchain_ollama",
    "# - ChatOllama is LangChain's official integration for Ollama models",
    "# - It provides a standardized interface to interact with Ollama",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Step 2: Initialize the ChatOllama model with configuration",
    "# Parameters:",
    "# - model: The Ollama model to use (must be pulled locally with: ollama pull rnj-1:8b)",
    "# - temperature: Controls randomness in responses (0.0 = deterministic, 1.0 = creative)",
    "# - Other optional params: top_k, top_p, num_predict (max tokens), etc.",
    "llm = ChatOllama(\n",
    "    model=\"rnj-1:8b\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Step 3: Define your prompt/question",
    "# - This is the input text sent to the model",
    "prompt = \"What is machine learning?\"\n",
    "\n",
    "# Step 4: Invoke the model (non-streaming)",
    "# - llm.invoke() sends the prompt and waits for the complete response",
    "# - Returns an AIMessage object with .content attribute containing the response",
    "response = llm.invoke(prompt)\n",
    "\n",
    "# Step 5: Print the response content",
    "# - response.content contains the generated text from the model",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================",
    "# Streaming Response Example",
    "# ============================================================",
    "\n",
    "# Re-import (not needed if same cell, but good for standalone execution)",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the model (same as before)",
    "llm = ChatOllama(\n",
    "    model=\"rnj-1:8b\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Define prompt",
    "prompt = \"Explain artificial intelligence in simple terms\"\n",
    "\n",
    "# ============================================================",
    "# Streaming Implementation",
    "# ============================================================",
    "# - llm.stream() returns an iterator that yields response chunks",
    "# - Each chunk is an AIMessageChunk object with .content attribute",
    "# - This is useful for showing partial results as they're generated",
    "# - More efficient for long responses as you don't wait for full output",
    "# ============================================================\n",
    "\n",
    "# Stream the response",
    "for chunk in llm.stream(prompt):\n",
    "    # chunk.content contains a portion of the generated text",
    "    # end=\"\" prevents adding newlines after each chunk",
    "    # flush=True ensures immediate display (real-time streaming)\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "# Print final newline after streaming completes",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "additional_options",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================",
    "# Additional Configuration Options",
    "# ============================================================",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Extended configuration options for ChatOllama:",
    "llm = ChatOllama(\n",
    "    model=\"rnj-1:8b\",\n",
    "    temperature=0.7,          # Controls creativity (0.0-1.0)\n",
    "    top_k=40,                # Top-k sampling parameter\n",
    "    top_p=0.9,               # Top-p (nucleus) sampling parameter\n",
    "    num_predict=512,         # Max tokens to generate\n",
    "    keep_alive=\"5m\",         # How long to keep model loaded in memory\n",
    "    base_url=\"http://localhost:11434\",  # Ollama server URL\n",
    ")\n",
    "\n",
    "# Using structured output with bind_tools (for function calling)\n",
    "# See LangChain documentation for more advanced patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
